{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "u8gwPUj3Owuz",
        "Ed25P60XOzdf",
        "31IzUIw_55Ot",
        "nUExziEW55eb",
        "tZ9Q6kLg55my",
        "XsgcTLHR555V",
        "3fGOat3clUL0",
        "UiT1ifx1k9f3",
        "5Xes6SjM56ST",
        "ZGvYMid056ZH",
        "1tux53Nm2uHz"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reshalfahsi/qa-gpt2-lora/blob/master/Question_Answering_GPT_2_PEFT_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question-Answering using GPT-2's PEFT with LoRA**"
      ],
      "metadata": {
        "id": "6ObWltxB533p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Important Libraries**"
      ],
      "metadata": {
        "id": "VqL8FTPH6C6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Install**"
      ],
      "metadata": {
        "id": "u8gwPUj3Owuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -LsSf https://astral.sh/uv/install.sh | sh"
      ],
      "metadata": {
        "id": "iTynQrK4749q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install -q --no-cache-dir --system transformers peft datasets\n",
        "!uv pip install -q --no-cache-dir --system bitsandbytes accelerate\n",
        "!uv pip install -q --no-cache-dir --system huggingface-hub\n",
        "!uv pip install -q --no-cache-dir --system evaluate"
      ],
      "metadata": {
        "id": "gD6mhBuR8M_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Import**"
      ],
      "metadata": {
        "id": "Ed25P60XOzdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import TrainingArguments, Trainer, BitsAndBytesConfig\n",
        "from transformers import DataCollatorForLanguageModeling, pipeline\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from datasets import load_dataset\n",
        "from evaluate import load as load_evaluate\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import warnings\n",
        "import random\n",
        "import torch\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "QIlZcpxy54K2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Configuration**"
      ],
      "metadata": {
        "id": "31IzUIw_55Ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"experiment\", exist_ok=True)\n",
        "os.makedirs(\"experiment/model\", exist_ok=True)\n",
        "EXPERIMENT_DIR = \"experiment/\""
      ],
      "metadata": {
        "id": "lWoj3eU0rrVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"gpt2\"\n",
        "FINETUNED_MODEL = os.path.join(EXPERIMENT_DIR, \"model\")\n",
        "DATASET_NAME = \"squad_v2\"\n",
        "METRIC_NAME = \"bleu\"\n",
        "MAX_ORDER = 1"
      ],
      "metadata": {
        "id": "YOiVQl1v55S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LORA_DROPOUT = 2e-1\n",
        "LORA_RANK = 4\n",
        "LORA_ALPHA = 8"
      ],
      "metadata": {
        "id": "zfwPDGyk7yAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_PER_DEVICE = 5\n",
        "GRADIENT_ACCUMULATION_STEP = 10"
      ],
      "metadata": {
        "id": "ZkJ_vnH270dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 5e-5\n",
        "SAMPLE_TEST_SIZE = 100"
      ],
      "metadata": {
        "id": "HdBf_9Bv71xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EVAL_STEP = int(2e2)\n",
        "LOGGING_STEP = int(2e2)\n",
        "SAVE_STEP = int(1e2)\n",
        "WARMUP_STEP = int(1e2)\n",
        "MAX_STEP = int(1.2e3)"
      ],
      "metadata": {
        "id": "rHyrEAEK7272"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKEN = 128\n",
        "MAX_LENGTH = 1024"
      ],
      "metadata": {
        "id": "EwoCs7Oe74ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ],
      "metadata": {
        "id": "OQRDLLX0CK6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset**"
      ],
      "metadata": {
        "id": "f_L1zmLe55Wz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load**"
      ],
      "metadata": {
        "id": "nUExziEW55eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_dataset_orig = load_dataset(DATASET_NAME)"
      ],
      "metadata": {
        "id": "Dslp3Ng-xnnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(\n",
        "    f\"The max model length is {tokenizer.model_max_length} for this model, \"\n",
        "    \"although the actual embedding size for GPT small is 768\"\n",
        ")\n",
        "print(\n",
        "    \"The beginning of sequence \"\n",
        "    f\"token {tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id)} \"\n",
        "    f\"token has the id {tokenizer.bos_token_id}\"\n",
        ")\n",
        "print(\n",
        "    \"The end of \"\n",
        "    f\"sequence token {tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id)} \"\n",
        "    f\"has the id {tokenizer.eos_token_id}\"\n",
        ")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\n",
        "    \"The padding \"\n",
        "    f\"token {tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id)} \"\n",
        "    f\"has the id {tokenizer.pad_token_id}\"\n",
        ")"
      ],
      "metadata": {
        "id": "ye5rwzhu55bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Utils**"
      ],
      "metadata": {
        "id": "tZ9Q6kLg55my"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(context, question, answer):\n",
        "    if len(answer[\"text\"]) < 1:\n",
        "        answer = \"Cannot Find Answer\"\n",
        "    else:\n",
        "        index = random.randint(0, len(answer[\"text\"])-1)\n",
        "        answer = answer[\"text\"][index]\n",
        "\n",
        "    prompt_template = (\n",
        "        f\"CONTEXT:\\n{context}\\n\"\n",
        "        f\"\\nQUESTION:\\n{question}\\n\"\n",
        "        f\"\\nANSWER:\\n{answer}\"\n",
        "    )\n",
        "\n",
        "    return prompt_template\n",
        "\n",
        "\n",
        "qa_dataset = qa_dataset_orig.map(\n",
        "    lambda samples: tokenizer(\n",
        "        create_prompt(\n",
        "            samples[\"context\"],\n",
        "            samples[\"question\"],\n",
        "            samples[\"answers\"],\n",
        "        ),\n",
        "        max_length=MAX_LENGTH,\n",
        "        truncation=True,\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "i3tLx0n555iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model**"
      ],
      "metadata": {
        "id": "evjfc6eX55uj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load**"
      ],
      "metadata": {
        "id": "XsgcTLHR555V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        ")"
      ],
      "metadata": {
        "id": "SoGcchip55y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Info**"
      ],
      "metadata": {
        "id": "3fGOat3clUL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = list(base_model.named_parameters())\n",
        "\n",
        "print(\n",
        "    'The GPT-2 model has {:} different named parameters.\\n'.format(len(params))\n",
        ")\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:2]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[2:14]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-2:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "metadata": {
        "id": "CWt8qF_9lTih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PEFT with LoRA**"
      ],
      "metadata": {
        "id": "UiT1ifx1k9f3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=LORA_RANK,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=[\n",
        "        \"c_attn\",\n",
        "        \"c_fc\",\n",
        "        \"c_proj\",\n",
        "    ],\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(base_model, lora_config)"
      ],
      "metadata": {
        "id": "_IBtbfGJk9lW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**"
      ],
      "metadata": {
        "id": "5Xes6SjM56ST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=qa_dataset[\"train\"],\n",
        "    eval_dataset=qa_dataset[\"validation\"],\n",
        "    args=TrainingArguments(\n",
        "        output_dir=EXPERIMENT_DIR,\n",
        "        per_device_train_batch_size=BATCH_PER_DEVICE,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEP,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=EVAL_STEP,\n",
        "        save_steps=SAVE_STEP,\n",
        "        warmup_steps=WARMUP_STEP,\n",
        "        max_steps=MAX_STEP,\n",
        "        logging_steps=LOGGING_STEP,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        fp16=True,\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        "    data_collator=DataCollatorForLanguageModeling(\n",
        "        tokenizer,\n",
        "        mlm=False,\n",
        "    ),\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "LQ6RgQR_jqpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(FINETUNED_MODEL)\n",
        "tokenizer.save_pretrained(FINETUNED_MODEL)"
      ],
      "metadata": {
        "id": "flHsQT2F81R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing**"
      ],
      "metadata": {
        "id": "ZGvYMid056ZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL)\n",
        "tokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "qa_metric = load_evaluate(METRIC_NAME)\n",
        "predictions = list()\n",
        "references = list()\n",
        "\n",
        "SEED = int(np.random.randint(2147483647))\n",
        "test_data = qa_dataset_orig[\"validation\"].shuffle(seed=SEED).select(\n",
        "    range(SAMPLE_TEST_SIZE)\n",
        ")\n",
        "\n",
        "\n",
        "for qa_data in tqdm(test_data):\n",
        "\n",
        "    answer = qa_data['answers']\n",
        "\n",
        "    if len(answer[\"text\"]) < 1:\n",
        "        answer = [\"Cannot Find Answer\"]\n",
        "\n",
        "    references.append(answer)\n",
        "\n",
        "    context = qa_data['context']\n",
        "    question = qa_data['question']\n",
        "\n",
        "    template = (\n",
        "        f\"CONTEXT:\\n{context}\\n\"\n",
        "        f\"\\nQUESTION:\\n{question}\\n\"\n",
        "        \"\\nANSWER:\\n\"\n",
        "    )\n",
        "\n",
        "    pred_index = len(template)\n",
        "\n",
        "    input = tokenizer(\n",
        "        template,\n",
        "        return_tensors='pt',\n",
        "        return_token_type_ids=False,\n",
        "    )\n",
        "    input = input.to(device='cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    with torch.cuda.amp.autocast():\n",
        "        model.enable_adapter_layers()\n",
        "        output_tokens_qa = model.generate(\n",
        "            **input,\n",
        "            max_new_tokens=MAX_TOKEN,\n",
        "            pad_token_id=tokenizer.pad_token_type_id,\n",
        "        )\n",
        "\n",
        "    answer = tokenizer.decode(\n",
        "        output_tokens_qa[0],\n",
        "        skip_special_tokens=True,\n",
        "    ).replace(template, '')\n",
        "    try:\n",
        "        answer = answer[:answer.find(\"\\n\\nANSWER:\\n\")]\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    predictions.append(answer)"
      ],
      "metadata": {
        "id": "r4LTcqPQ4qM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = qa_metric.compute(\n",
        "    predictions=predictions,\n",
        "    references=references,\n",
        "    max_order=MAX_ORDER,\n",
        ")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "5LGqAQCWhCaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inference**"
      ],
      "metadata": {
        "id": "hyJ9D9JK56hP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Utils**"
      ],
      "metadata": {
        "id": "1tux53Nm2uHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL)\n",
        "tokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "EcXys5U456lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_inference(context, question):\n",
        "\n",
        "    template = (\n",
        "        f\"CONTEXT:\\n{context}\\n\"\n",
        "        f\"\\nQUESTION:\\n{question}\\n\"\n",
        "        \"\\nANSWER:\\n\"\n",
        "    )\n",
        "\n",
        "    # turn the input into tokens\n",
        "    input = tokenizer(\n",
        "        template,\n",
        "        return_tensors='pt',\n",
        "        return_token_type_ids=False,\n",
        "    )\n",
        "    # move the tokens onto the GPU, for inference\n",
        "    input = input.to(device='cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # make an inference\n",
        "    with torch.cuda.amp.autocast():\n",
        "        model.enable_adapter_layers()\n",
        "        output_tokens_qa = model.generate(\n",
        "            **input,\n",
        "            max_new_tokens=MAX_TOKEN,\n",
        "            pad_token_id=tokenizer.pad_token_type_id,\n",
        "        )\n",
        "\n",
        "    result = tokenizer.decode(output_tokens_qa[0], skip_special_tokens=True)\n",
        "    try:\n",
        "        result = result[\n",
        "            :result.find(\n",
        "                \"\\n\\nANSWER:\\n\",\n",
        "                result.find(\"\\n\\nANSWER:\\n\") + 1,\n",
        "            )\n",
        "        ]\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    result = result.replace(\n",
        "        \"CONTEXT:\", \"**CONTEXT:**\"\n",
        "    ).replace(\n",
        "        \"QUESTION:\", \"**QUESTION:**\"\n",
        "    ).replace(\n",
        "        \"ANSWER:\", \"**ANSWER:**\"\n",
        "    )\n",
        "\n",
        "    # display results\n",
        "    display(Markdown(\"\\n# Question-Answering with GPT-2 \\n\"))\n",
        "    display(Markdown(\"\\n  \\n\"))\n",
        "    display(\n",
        "        Markdown(\n",
        "            (result)\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "F1MXw5Jd2uN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Run**"
      ],
      "metadata": {
        "id": "t_A-t7Cd2wPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT = \"The trophy doesn't fit into the brown suitcase because it's too small.\" # @param {type:\"string\"}\n",
        "QUESTION = \"What is too small?\"  # @param {type:\"string\"}\n"
      ],
      "metadata": {
        "id": "N72F2ocK56tg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ANSWER:\n",
        "make_inference(CONTEXT, QUESTION)"
      ],
      "metadata": {
        "id": "V4GLRA9L6_H5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}